import numpy as np
import matplotlib . pyplot as plt
from sklearn . datasets import fetch_openml
from sklearn . linear_model import LogisticRegression
from sklearn . model_selection import GridSearchCV ,train_test_split
from sklearn . pipeline import Pipeline
from sklearn . preprocessing import StandardScaler
from sklearn . metrics import accuracy_score
from sklearn . utils import check_random_state
# data loading
train_samples = 500
X , y = fetch_openml ('mnist_784', version =1 , return_X_y = True ,as_frame = False )
random_state = check_random_state (0)
permutation = random_state . permutation ( X . shape [0])
X = X [ permutation ]
y = y [ permutation ]
X = X . reshape (( X . shape [0] , -1) )
X_train , X_test , y_train , y_test = train_test_split (X , y ,train_size = train_samples , test_size =100)

lasso_logistic_pipeline = Pipeline([
('scaler', StandardScaler()), # Standardize feature
('lasso_logistic', LogisticRegression(penalty='l1', solver='liblinear', multi_class='auto'))])
# Define the parameter grid for hyperparameter tuning
param_grid = {'lasso_logistic__C': np.logspace(-2, 2, 9)}

grid_search = GridSearchCV(lasso_logistic_pipeline, param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
# Get the best value of C
best_C = grid_search.best_params_['lasso_logistic__C']
print('Best C:', best_C)

# Predict on the test set
y_pred = grid_search.predict(X_test)

# Extract the hyperparameter values and corresponding mean test scores
C_values = np.logspace(-2, 2, 9)
mean_test_scores = grid_search.cv_results_['mean_test_score']
# Plot the mean test score against C
plt.semilogx(C_values, mean_test_scores, marker='o')
plt.xlabel('C')
plt.ylabel('Accuracy')
plt.title('Mean Test Score vs. C')
plt.grid(True)
plt.show()

from sklearn.metrics import confusion_matrix, precision_score, recall_score,f1_score
# Make predictions on the test set
y_pred = grid_search.predict(X_test)
confusion_matrix = confusion_matrix(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='macro')
recall = recall_score(y_test, y_pred, average='macro')
f1 = f1_score(y_test, y_pred, average='macro')
print('Accuracy:', accuracy)
print('Confusion Matrix:\n', confusion_matrix)
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1)
